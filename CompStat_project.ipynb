{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest: middle ground between interpretability and accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Simulation Study](#simulation_study)\n",
    "    1. [Data Analysis](#data_analysis)\n",
    "    2. [Simulation](#simulation)\n",
    "    3. [Simulation Results](#simulation_results)\n",
    "3. [Empirical Application](#empirical_application)\n",
    "4. [References](#References)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"pROC\")\n",
    "install.packages(\"dplyr\")\n",
    "install.packages(\"splitstackshape\")\n",
    "install.packages(\"moments\")\n",
    "install.packages(\"sn\")\n",
    "\n",
    "library(rpart)\n",
    "library(randomForest)\n",
    "library(pROC)\n",
    "library(dplyr)\n",
    "library(splitstackshape) \n",
    "library(moments)\n",
    "library(sn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction <a class=\"anchor\" id=\"introduction\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree can be a very useful tool when the structure of the data analysed is known to be highly non-linear and complex. It has also an advantage of simplicity of explanation: decision tree can be displayed graphically and is easy to interpret. The last point is particularly important when the results of an analysis are used by people less familiar with regression and classification techniques. The significant disadvantage of the method is the lack of predictive accuracy when compared to other methods like linear regressions, support vector maschines and unsupervised learning techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"random_forest.png\" width=1000 height=1000 style=\"float:center\" />\n",
    "\n",
    "image source: https://towardsdatascience.com/random-forests-and-decision-trees-from-scratch-in-python-3e4fa5ae4249"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an approach that is based on a Decision Tree method and aimed to increase prediction accuracy. It is based on several decorrelated decision trees, each trained on a different random sub-sample (e.g. bootstrapped training samples). The prediction from the Random Forest is then an average of predictions of the decision trees in case of regression or the class with the highest occurence in case of classification. The way decorrelation of the trees is achieved is by allowing to consider only subset of the features at each split of a tree, with each subset being randomly chosen. This means that at some splits it will not be possible to use the strongest predictors. Corespondingly, the model's prediction will be less dependent on a subsample of the strongest features and therefore will be less volatile. This approach leads to lower variance and as a result to higher predictive power. The higher predictive power, however, comes at the cost of lower interpretability as simple graph of a single Decision Tree and intuitive explanation based on that are no more available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"balance.png\" width=500 height= 500 />\n",
    "\n",
    "image source: https://mc.ai/interpretability-vs-accuracy-the-friction-that-defines-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to notice that even though interpretability of the Random Forest is lower when compared to the Decision Tree, it remains high relative to such methods as support vector maschine (SVM), neural network, etc. The latter is basically a black box, and therfore much less interpretable and requires additional analysis to get any information other than predictions themselves. It is possible, for example, to retrieve an information about the features' importance using Gini index or residual sum of squares in case of Random Forest (this functionality often comes in the same programming package), whereas additional method (like Decision Tree) is needed to get any kind of additional information in the case of Neural Network or SVM. However, just like in the case of Decision Tree and Random Forest, Random Forest being more interpretable in most of the cases will lack acuracy precision when compared to the methods mentioned.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, in the case of non-linear and complex structure of data, the choice between these methods is nothing else than a choice between the interpretability and prediction precision. The right choice of balance is highly dependent on the goal of an analysis and the nature of data in question. Therefore for the appropriate data and goal, the Random Forest can be the golden middle between the prediction accuracy of Neural Network, SVM and the interpretability of Decision Tree.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I use data that I believe to be suitable for the Random Forest method, namely, the data from direct marketing campaigns of a Portuguese retail bank. Unfortunately, this is not exactly the same data set (some of the features were not included due to the privacy concerns) that was used in the paper this project based on: \"A data-driven approach to predict the success of bank telemarketing\" (Moro S., Cortez P. and Rita P., 2014). The paper is available at https://www.sciencedirect.com/science/article/abs/pii/S016792361400061X .The dataset is public and available for research at http://archive.ics.uci.edu/ml/datasets/Bank+Marketing# ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simulation Study <a class=\"anchor\" id=\"simulation_study\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Telemarketing campaign is a method of reaching out to potential customers remotely with the goal of convincing to buy a product or service. In our case it is a phone calls to potential clients of a Portugese retail bank with the goal of convincing to open a deposit sell. This method is one of the most widely used as it allows to contact a large number of people, but in order to increase the effeciency further only those customers that are the most likely to purchase a product should be contacted. To achieve this managers often use Decision Support Systems, software-based programmes processing large amounts of data in order to help businesses and organisations in the decision-making process. Data mining is one of the techniques that is widely used, with the most frequent task being classification. In the current study the question is to determine whether the client is likely or not to open a deposit sell based on the personal characteristics as well as available information about current economic situation. However, this is not the only goal of the analysis, the main determinants of the result are also of interest for the managers since they will help to create an appropriate policy and in doing so affect future client behaviour. The goal is, therefore, to construct a model that will have a high level of accuracy as well as interpretability. This is the task that is often faced and that poses a question of an apropriate balance of accuracy and interpretability and, accordingly, model. \n",
    "\n",
    "Authors of the paper are considerng following models for this task: Logistic Regression, Decision Tree, Support Vector Machine (SVM) and Neural Networks. While their analysis shows that the first two of the model lack predictive power, it also becomes apparent that the last two models are hard to interpret. In particular the model that achieved the best accuracy, Neural Networks, is in an essence a black box. To retrieve some of the information additional tools were used such as Data-based Sensitivity Analysis algorithm and Decision Tree applied to the output responses of the fitted model (S.Moro, et al, 2014). In this project I analyse an alternative method that I believe, given the data in question, could be a golden middle between accuracy of the Neural Networks, SVM and the interpretability of Logistic regression and Decision Tree.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two reasons I believe this data is appropriate for the Random Forest analysis\n",
    "\n",
    "1. There is a high chance that the structure of data is non-linear and complex. This is supported by the results of the analysis performed in the paper, where the lowest predictive accuracy was achieved by Logistic regression, even when compared to Decision Tree which is known to have a low predictive power on the one hand and a good fit for a non-linear, complex structure on the other. These two considerations led me to believe that underlying structure is non-linear. Moreover, from the data analysis multicollinearity problem becomes apparent and while it affects Random Forest analysis, the situation for any linear regression is much worse. Therefore Random Forest is more preferable that any other approach based on a linear regression such as Logistic Regression.\n",
    "\n",
    "2. The nature of the data. The approach taken by authors of the paper is to achieve a high reaisticity by constracting a rolling window evaluation. In the reality, the new information becomes available regularly and at the same time the behaviour of the client (and the probability of consuming a product) changes with the time following changes in the environment. I keep this approach and construct my version of the \"rolling window\". I believe that Random Forest is more suitable when compared to the Decision Tree, as it will be more efffecient in incorporating new information to make a better prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of the analysis that follows is to show that Random Forest offers a considerable improvement in the prediction accuracy when compared to the Decision Tree. I do not consider Logistic Regression for now since it performed poorly when compared to the Decision Tree. I also do not consider SVM and Neural Networks since the idea is that Random Forest outperforms them in terms of interpretability and not accuracy (I assume that both of these techniques will achieve a higher predictive accuracy). To demonstrate a higher interpretability, I will apply \"in-built\" fuctions using which one can easily access, for example,  the feature importance data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.A Data analysis <a name=\"data_analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon closer inspection of the data, it becomes apparent that it differs considerbly from the data used in the study by Moro S., Cortez P. and Rita P. (2014) It affects the study and most importantly results that are no more comparable to those ones presented in the paper. \n",
    "\n",
    "In what follows I retrieve the data information that will be used later on in the simulation study, whenever necessary I add comments directly to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data and do some cleaning\n",
    "df <- read.csv(\"bank-additional-full.csv\", sep=\";\")\n",
    "df[df == \"unknown\"] <- NA\n",
    "df <- na.omit(df)\n",
    "df$duration <- NULL # information related to the last call (the dependent variable in the setting)\n",
    "df$y[df$y == \"yes\"] <- 1\n",
    "df$y[df$y == \"no\"] <- 0\n",
    "df$y <- as.numeric(df$y)\n",
    "df$pdays[df$pdays == 999] <- 0 # 999 means that client was not previously contacted\n",
    "vector.quali <- c('job', 'marital', 'education', 'default', 'housing', 'loan', 'contact',\n",
    "                  'month', 'day_of_week', 'poutcome', 'y')\n",
    "df[vector.quali] <- lapply(df[vector.quali], factor)\n",
    "df[c(\"age\", \"campaign\", \"pdays\", \"previous\")] <- sapply(df[c(\"age\", \"campaign\", \"pdays\", \"previous\")],as.numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fraction (in percentage) of each categorical variable\n",
    "x <- lapply(df[, c('job', 'marital', 'education', 'default', 'housing', 'loan', 'contact',\n",
    "                       'month', 'day_of_week', 'poutcome', 'y')], table)\n",
    "\n",
    "neat.table <- function(x, name){\n",
    "  xx <- data.frame(x)\n",
    "  names(xx) <- c(\"Value\", \"Count\")\n",
    "  xx$Fraction <- with(xx, Count*100/sum(Count))\n",
    "  data.frame(Variable = name, xx)\n",
    "}\n",
    "\n",
    "do.call(rbind, lapply(seq_along(x), function(i)neat.table(x[i], names(x[i]))))\n",
    "\n",
    "# information on \"time-invariant\" continuous variables: age, campaign, pdays, previous\n",
    "hist(df$previous)\n",
    "hist(df$age)\n",
    "hist(df$campaign)\n",
    "hist(df$pdays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains both categorical and continuous variables. As can be seen from the graphs above some of the continuous variables have skewed distributions. The variables can be divided further into two subgroups: personal information variables (e.g. profession, information on whether person does or does not have a loan, housing mortgage, etc.) and variables describing economic environment at that point of time (e.g. number of employees, consumer confidence index, etc.). The dependent variable is a boolean: yes/no response to wether the person has decided to open a deposit sell. In the next subsection I describe my approach to data simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.B Simulation <a name=\"simulation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I subdevided data simulation in 4 parts according to the nature of the data and challanges it presents. \n",
    "\n",
    "1. Time-dependent variables \n",
    "\n",
    "    The analysis of Moro et al. (2014) shows the economic and social indicators are the main determinants of the response variable. It is important to notice that these are also the variables that tend to change with the time (as opposed to, for example, profession). And as was stated before this is one of the reasons I believe Random Forest will perform better when compared to the Decision Tree. Hence, it was important to incorporate the time trend into the simulation. In order to make simulation as realistic as possible I decided to use the real-world data on economic indicators of that period of time.  These features are:\n",
    " * emp.var.rate: employment variation rate - quarterly indicator \n",
    " * cons.price.idx: consumer price index - monthly indicator \n",
    " * cons.conf.idx: consumer confidence index - monthly indicator \n",
    " * euribor3m: euribor 3 month rate - daily indicator \n",
    " * nr.employed: number of employees - quarterly indicator \n",
    " \n",
    "   I retrieved them from the data and in order to make all quarters equally represented performed some data manipulation. Unfortunately, there is no information about the time period when each call was made, the only available information is that the data ordered by date (from May 2008 to November 2010). The way I attempted to overcome this obstacle is: after choosing all unique combinations of above mentioned variables, I have randomly chosen seven data points corresponding  to each value of a mothly indicator (consumer confidence index). Out of these I have randomly chosen 7 data points corresponding to each value of the quarterly indicator (number of employees).This left me with 77 observations (7 multiplied by 11, number of quarters). In the simulated data each data point will be represented equally and therfore there is a requirement for the total number of observations to be a factor of 77. The choice of the monthly indicator variable was arbitrary since both monthly indicators contain a unique value corresponding to each month. Quarterly indicator variable wasn't an arbitrary choice since number of employees indicator was the only variable containing a unique value for each quarter. The number 7 was chosen because it is the minimum number of unique data points per each monthly indicator and as I tried to achieve the higher data variation among daily, monthly and quarterly indicators, it did not to make sense to choose value lower than that (choosing the value higher on the other hand would have resulted in the unequal representation of some quarters). In addition to the variables mentioned above I have also added a corresponding month as it is directly related to the values of the monthly indicators and creat a high level of collinearity which I also wanted to include in my simulated data in order for it to be more representative of the real-world data.\n",
    "   \n",
    "2. Categorical variables\n",
    "\n",
    "    The way I simulated categorical variables (with the exception of month) is relatively straightforward: I gathered information on the fraction of each response in the original data and used these to represent probabilities of getting these responses in the simulated data.\n",
    "    \n",
    "3. Continuous variables\n",
    "\n",
    "    As was pointed out before continuous variable have a skewed distributions and taking logarithms unfortunately did not help much. To simulate these variables I used an sn R package, which allowed me to specify the skeweness and kurtosis of the original distributions. This, however, also posed some challenges as there exist boundaries for possible combinations of these two values. The feasibilty region is nicely represented in the fig. 1 in the paper by Arellano-Valle R. B. and Azzalini A. (2013) \"The centred parameterization and related quantities of the skew-t distribution\" (on a picture below: $\\gamma_1$ is a skeweness and $\\gamma_2$ is a kurtosis). Where it was impossible to use the original values I used the closest feasible value.\n",
    "    \n",
    "    <img src=\"kurtosis.jpg\" width=400 height= 400 />\n",
    "\n",
    "    image source: Arellano-Valle R. B. and Azzalini A. (2013) \"The centred parameterization and related quantities of the skew-t distribution\"\n",
    "    \n",
    "4. Response variable\n",
    "    \n",
    "    To simulate a response variable I decided to utilize the relationship within the original data set. I grow a Classification Tree on the original data and then apply it to the explanatory variables in the simulated data set. As a result I obtain probabilities of the output being a success (i.e. \"yes\"), to which I add some noise (random draw from normal distribution with mean 0 and standard deviation of 0.2). Finally, I classify a simulated response variable to be success if the resulted probability is larger than a certain threshold (\"prob.succcess\" in the function below, default is 0.5). For the comparison reasons I add a variable y.true, which is assigned success if the probability of it without noise added is higher than 0.5. \n",
    "    This approach has a considerable drawbacks as person looses control over the structure of the simulated dataset, but it helps to achieve a goal of giving a non-linear structure and makes simulated data closer to the real-world processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fuction for the data simulation.\n",
    " \n",
    " input:  N (integer): total number of observations to be simulated; should be factor of 77.\n",
    "         dataset (dataframe): original data set.\n",
    "         prob.success (numerical): probability threshold for the response variable determening success/failure.\n",
    "         \n",
    " output: dataframe: simulated data set with additional columns of y.prob.true (true probability of success), \n",
    "                    y.true (assigned success, i.e 1, if y.prob.true > 0.5), e (error rate), y.prob (sum of y.prob.true and error rate).\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "df.simulation <- function(N, dataset, prob.success=0.5){\n",
    "  \n",
    "  # \"time-invariant\" variables\n",
    "  # categorical:\n",
    "  job <- sample(c(\"housemaid\", \"services\", \"admin.\", \"technician\", \"blue-collar\", \"unemployed\", \"retired\", \n",
    "                  \"entrepreneur\", \"management\", \"student\", \"self-employed\"), N, replace=TRUE, \n",
    "                prob=c(0.027, 0.09, 0.287, 0.1795, 0.186, 0.024, 0.0399, 0.0357, 0.0758, 0.02, 0.0358))\n",
    "  marital <- sample(c(\"divorced\", \"married\", \"single\"), N, replace=TRUE, prob=c(0.1165, 0.5737, 0.3097))\n",
    "  education <- sample(c(\"basic.4y\", \"basic.6y\", \"basic.9y\", \"high.school\", \"illiterate\", \n",
    "                        \"professional.course\", \"university.degree\"), N, replace=TRUE, \n",
    "                      prob=c(0.078, 0.046, 0.14, 0.25, 0.0004, 0.14, 0.342))\n",
    "  default <- sample(c(\"no\", \"yes\"), N, replace=TRUE, prob=c(99.99, 0.01))\n",
    "  housing <- sample(c(\"no\", \"yes\"), N, replace=TRUE, prob=c(45.8, 54.2))\n",
    "  loan <- sample(c(\"no\", \"yes\"), N, replace=TRUE, prob=c(84.36, 15.64))\n",
    "  contact <- sample(c(\"cellular\", \"telephone\"), N, replace=TRUE, prob=c(67.05, 32.95))\n",
    "  day_of_week <- sample(c(\"fri\", \"mon\", \"thu\", \"tue\", \"wed\"), N, replace=TRUE, prob=c(0.188, 0.206, 0.209, \n",
    "                                                                                      0.195, 0.2))\n",
    "  poutcome <- sample(c(\"failure\", \"nonexistent\", \"success\"), N, replace=TRUE, prob=c(0.114, 0.847, 0.39))\n",
    "  #numerical:\n",
    "  age.params <- cp2dp(c(mean(dataset$age), sd(dataset$age), skewness(dataset$age)), \"SN\")\n",
    "  age <- c(rsn(N, dp=age.params))\n",
    "  campaign.params <- cp2dp(c(mean(dataset$age), sd(dataset$age), skewness(dataset$age), \n",
    "                             kurtosis(dataset$campaign)), \"ST\")\n",
    "  campaign <- c(rst(N, dp=campaign.params))\n",
    "  pdays.params <- cp2dp(c(mean(dataset$pdays), sd(dataset$pdays), 3.3, \n",
    "                             kurtosis(dataset$pdays)), \"ST\")\n",
    "  pdays <- c(rst(N, dp=pdays.params))\n",
    "  previous.params <- cp2dp(c(mean(dataset$previous), sd(dataset$previous), 2.5, \n",
    "                             kurtosis(dataset$previous)), \"ST\")\n",
    "  previous <- c(rst(N, dp=previous.params))\n",
    " \n",
    "  #create dataframe out of \"time-invariant\" variables\n",
    "  df.nottime.var <- cbind(job, marital, education, default, housing, loan, contact, day_of_week, \n",
    "                          poutcome, age, campaign, pdays, previous)\n",
    "  colnames(df.nottime.var) <- c(\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\",\n",
    "                                \"contact\", \"day_of_week\", \"poutcome\", \"age\", \"campaign\", \n",
    "                                \"pdays\", \"previous\")\n",
    "  df.nottime.var <- as.data.frame(df.nottime.var)\n",
    "  \n",
    "  # time-dependent variables\n",
    "  # Create a data frame which contains time-dependent variables. \n",
    "  # The values are chosen randomly  for each quarter from the original data set. \n",
    "  # Each quarter contains the same number of people interviewed (factor of 7).\n",
    "  df.time.unique <- unique(dataset[,c('month', 'emp.var.rate','nr.employed','cons.conf.idx','cons.price.idx','euribor3m')])\n",
    "  df.time.strat <- stratified(stratified(df.time.unique, \"cons.conf.idx\", 7), \"nr.employed\", 7)\n",
    "  n.per.time <- N/nrow(df.time.strat)\n",
    "  df.time.var <- df.time.strat[rep(seq_len(nrow(df.time.strat)), each = n.per.time), ]\n",
    "  \n",
    "  # dataframe of all explanatory variables\n",
    "  df.full <- cbind(df.nottime.var, df.time.var)\n",
    "  df.full$age <- as.integer(df.full$age)\n",
    "  df.full$campaign <- as.integer(df.full$campaign)\n",
    "  df.full$pdays <- as.integer(df.full$pdays)\n",
    "  df.full$previous <- as.integer(df.full$previous)\n",
    "  \n",
    "  # use the whole empirical dataset to create tree and produce y.true for the simulated data set.\n",
    "  tree.fit <- rpart(y~., data=dataset, method=\"class\")\n",
    "  y.predict.matrix <- predict(tree.fit, newdata=df.full, type=\"prob\")\n",
    "  df.full$y.prob.true <- as.vector(y.predict.matrix[,2])\n",
    "  df.full$y.true <- as.factor(ifelse(df.full$y.prob.true > 0.5, 1, 0))\n",
    "  df.full$e <- rnorm(n=N, mean=0, sd=0.2)\n",
    "  df.full$y.prob <- df.full$y.prob.true + df.full$e\n",
    "  df.full$y <- as.factor(ifelse(df.full$y.prob > prob.success, 1, 0))\n",
    "\n",
    "  return(df.full)\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was noted before there are two points I want to address in the data simulation part. The one that is related to the non-linear structure of the data was incorporated in the data simulation process directly. The second point is conderned with the flow of the data and will be addressed in the evaluation part of the analysis. Moro S. et al. (2014) created a rolling window evaluation scheme performing several updates and discarding old data. The scheme consists of the following steps (see figure below):\n",
    "1. A training window of consecutive observations of size W (i.e. training sample) is used to train model  \n",
    "2. The next K observations are used to test the model\n",
    "3. All K observations used for testing in step (2.) are added to the training model, while the first K observations are discarded\n",
    "4. The process repeats until there are no observations left to create a new test set.\n",
    "\n",
    "I implemeted the same scheme in the function below. I use Area Under the Curve (AUC) to evaluate the prediction accuracy of the Decision Tree and Random Forest. It is possible to choose the percentage of the data to be used for test set and to choose the number of runs (data updates) to be performed. The average AUCs are then calculated and returned in the form of a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rolling_window.png\" width=500 height= 500 />\n",
    "\n",
    "image source: S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing (2014) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for calculating  test Area Under the Curve (AUC) scores \n",
    "of Decision Tree and Random Forest applied to the simulated data set\n",
    "using a rolling window evaluation. \n",
    "\n",
    "input: dataset (dataframe): simulated data set\n",
    "       K (integer): number of runs, i.e. training and test sets updates\n",
    "       test.size (numerical): fraction of the data used as a test set; 0 < test.size < 1\n",
    "\n",
    "output: dataframe: results for the average test Area Under the Curve (AUC)\n",
    "\n",
    "\"\"\"\n",
    "sim.rolling.window.evaluation <- function(dataset, K, test.size){\n",
    "  \n",
    "  # calculate sizes of training and test sets\n",
    "  r <- (1-test.size)/test.size\n",
    "  N <- nrow(dataset) \n",
    "  n.train <- N*r/(K+r)\n",
    "  n.test <- (N-n.train)/ K  \n",
    "  \n",
    "  # additional data needed for simulation evaluation\n",
    "  y.true <- dataset$y.true\n",
    "  y.true.train <- head(x=y.true, n=n.train)\n",
    "  y.true.test.all <- tail(x=y.true, n=(N-n.train))\n",
    "  y.true.test <- head(x=y.true.test.all,n=n.test)\n",
    "  # drop unnecessary columns\n",
    "  dataset$y.prob.true <- NULL\n",
    "  dataset$e <- NULL\n",
    "  dataset$y.prob.fin <- NULL\n",
    "  dataset$y.true <- NULL\n",
    "  \n",
    "  # create: initial training set (train),\n",
    "  #         set from which test sets are drawn (test.all), \n",
    "  #         first test set  (test)\n",
    "  \n",
    "  train <- head(x=dataset, n=n.train)\n",
    "  y.train <- train$y\n",
    "  predictors.train <- train[, names(train)!= \"y\"]\n",
    "  test.all <- tail(x=dataset, n=(N-n.train))\n",
    "  test <- head(x=test.all,n=n.test)\n",
    "  y.test <- test$y\n",
    "  predictors.test <- test[, names(test)!= \"y\"]\n",
    "  \n",
    "  \n",
    "  # loop for 1) fitting each model using a train set, calculating respective test AUC \n",
    "  #          2) updating train and test data sets\n",
    "  #          3) calculating test AUC\n",
    "  \n",
    "  AUC.tree.appended <- c()\n",
    "  AUC.forest.appended <- c()\n",
    "  \n",
    "  for (i in 2:K){\n",
    "    \n",
    "    forest.fit <- randomForest(x=predictors.train, y=y.train)\n",
    "    y.matrix.forest <- predict(object=forest.fit, newdata=predictors.test, type=\"prob\")\n",
    "    y.data.forest <- cbind.data.frame(as.vector(y.matrix.forest[,2]), y.true.test)\n",
    "    AUC.forest.result <- auc(data=y.data.forest, response=y.data[,2], predictor=y.data[,1])\n",
    "    AUC.forest.appended <- append(AUC.forest.appended, AUC.forest.result)\n",
    "    \n",
    "    tree.fit <- rpart(y~., data=train, method=\"class\")\n",
    "    y.matrix.tree <- predict(tree.fit, newdata=test, type=\"prob\")\n",
    "    y.data.tree <- cbind.data.frame(as.vector(y.matrix.tree[,2]), y.true.test)\n",
    "    AUC.tree.result <- auc(data=y.data.tree, response=y.data[,2], predictor=y.data[,1])\n",
    "    AUC.tree.appended <- append(AUC.tree.appended, AUC.tree.result)\n",
    "    \n",
    "    if (nrow(test.all) >= n.test){\n",
    "      \n",
    "      train <- tail(train, -n.test)\n",
    "      train <- rbind(train, test)\n",
    "      test.all <- tail(test.all, -n.test)\n",
    "      test <- head(x=test.all,n=n.test)\n",
    "      y.train <- train$y\n",
    "      predictors.train <- train[, names(train)!= \"y\"]\n",
    "      y.test <- test$y\n",
    "      predictors.test <- test[, names(test)!= \"y\"]\n",
    "      \n",
    "      y.true.test.all <- tail(y.true.test.all, -n.test)\n",
    "      y.true.test <- head(x=y.true.test.all,n=n.test)\n",
    "      \n",
    "    }\n",
    "    \n",
    "  }\n",
    "  \n",
    "  ave.AUC.forest <- mean(AUC.forest.appended)\n",
    "  ave.AUC.tree <- mean(AUC.tree.appended)\n",
    "  \n",
    "  df.AUC <- data.frame(matrix(ncol=2, nrow=2))\n",
    "  colnames(df.AUC) <- c(\"Method\",\"ave.AUC\")\n",
    "  df.AUC$Method[1] <- \"Decision Tree\"\n",
    "  df.AUC$Method[2] <- \"Random Forest\"\n",
    "  df.AUC$ave.AUC[1] <- ave.AUC.tree\n",
    "  df.AUC$ave.AUC[2] <- ave.AUC.forest\n",
    "  \n",
    "  return(df.AUC)\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.C Simulation Results <a name=\"simulation_results\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subsections presents results that one can obtain using the simulation and evaluation functions described in the previous subsection and discusses the possible reasons for the large discrepency of the results obtained in this study and the ones obtained by authors of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data set\n",
    "sim.data <- df.simulation(N=7700, dataset=df)\n",
    "\n",
    "# Get the evaluation results\n",
    "sim.results <- sim.rolling.window.evaluation(df.trial, 10, 0.10)\n",
    "print(sim.results)\n",
    "\n",
    "# remove unnecessary columns for the further analysis\n",
    "sim.data$y.prob.true <- NULL\n",
    "sim.data$e <- NULL\n",
    "sim.data$y.prob.fin <- NULL\n",
    "sim.data$y.true <- NULL\n",
    "\n",
    "# fit Classification tree and get summary\n",
    "tree.fit <- rpart(y~., data=sim.data, method=\"class\")\n",
    "summary(tree.fit)\n",
    "plot(tree.fit)\n",
    "text(tree.fit)\n",
    "\n",
    "#check for multicollinearity of a subset of features\n",
    "df.col <- data.frame(sim.data$emp.var.rate, sim.data$euribor3m, sim.data$nr.employed)\n",
    "df.col[] <- lapply(df.col, as.numeric)  \n",
    "chisq <- chisq.test(table(unlist(df.col)))\n",
    "print(chisq)\n",
    "\n",
    "df.col.add <- data.frame(sim.data$emp.var.rate, sim.data$euribor3m, sim.data$nr.employed, sim.data$cons.conf.idx,  sim.data$cons.price.idx)\n",
    "df.col.add[] <- lapply(df.col.add, as.numeric)  \n",
    "chisq.add <- chisq.test(table(unlist(df.col.add)))\n",
    "print(chisq.add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I obtain the same high prediction accuracy results for both, Decision Tree and Random Forest. This was not expected since accoring to the results presented in the paper Decision Tree performed poorly when compared to the other techniques, obtaining AUC of 0.757. Moreover, even the method that performed the best in case of Moro S., et al (2014), i.e. Neural Networks, achieved AUC of only 0.794. However, upon the closer inspection of the data, it becomes apparent that the majority of the features are different in two datasets, the one used in this study and the one used by the authors of the paper. Thus, some of the most important features such as direction of the call and the number of days since the agent login was created are absent in the data analysed. Moreover, only 4 of the most important features according to the Moro S. et al (2014) are presented (euribor 3 month rate, employment variation rate, number of employees, month of the call). As one can see from the summary of the Classification tree fit, all 4 are indeed among 8 most important variables. With 3 of them being economic indicators and the forth being a month in which those indicators were recorded and the cal was made, the chances of multicollinearity are high. This is confirmed by the chi-square test shown above. Another two variables among the most important ones are economic indicators as well. I performed chi-square test for all economic indicators included in the list of the most important variables and once again got a high statistically significant result. Only two of the most important outcomes are not economic indicators and are not related to the rest of features due to the way the data was simulated. Thus, due to the multicollinearity, it was possible to pick one or some of the economic indicators and achieve a good prediction accuracy (hence, small Classification Tree is grown). I believe this simplified task when compared to the classification problem on a data set used by Moro S. et al (2014). Since Classification Tree achieves a high accuracy rate, there is no room for Random Forest to outperform it. In this case Decision Tree, being simple to interpret and achieving a high accuracy rate, is of course an obvious choice for the task. However, this result should be taken with the grain of salt as the data set that was considered in this project is clearly not representative of the real-world data used by Moro S. et al (2014) and the similarity of the results of Decision Tree and Random Forest should make one question the correctness of the implemetation. \n",
    "\n",
    "Due to the way data was simulated it was expected that most of the difference between Decsion Tree and Random Forest would have come from the rolling window evaluation. It was expected that Random Forest will outperform Decision Tree in terms of accuracy prediction as test data sets contain new information from the days, months and quarters ahead of the training set. Decision Tree is not suited for making predictions based on out-of-sample values of features and therfore should have performed worse. This, however, was not supported by the results of the study. One of possible reasons is that differents between daily/mothly/quarterly indicators was not high enough from sample to sample to qualify for a \"ou-of-sample values\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Empirical application <a class=\"anchor\" id=\"empirical_application\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the simulation study was essentialy based on the original data similar results are expected. For this section an additional function for rolling window evaluation was created with the sole difference being a comparison of predicted class to the the one given in data instead of the simulated true response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for calculating  test Area Under the Curve (AUC) scores \n",
    "of Decision Tree and Random Forest applied to the original data set\n",
    "using a rolling window evaluation. \n",
    "\n",
    "input: dataset (dataframe): data set\n",
    "       K (integer): number of runs, i.e. training and test sets updates\n",
    "       test.size (numerical): fraction of the data used as a test set; 0 < test.size < 1\n",
    "\n",
    "output: dataframe: results for the average test Area Under the Curve (AUC)\n",
    "\n",
    "\"\"\"\n",
    "rolling.window.evaluation <- function(dataset, K, test.size){\n",
    "  \n",
    "  # calculate size of a training set (n.train)\n",
    "  r <- (1-test.size)/test.size\n",
    "  N <- nrow(dataset) \n",
    "  n.train <- N*r/(K+r)\n",
    "  \n",
    "  \n",
    "  # create: initial training set (train),\n",
    "  #         set from which test sets are drawn (test.all), \n",
    "  #         first test set  (test)\n",
    "  \n",
    "  train <- head(x=dataset, n=n.train)\n",
    "  y.train <- train$y\n",
    "  predictors.train <- train[, names(train)!= \"y\"]\n",
    "  test.all <- tail(x=dataset, n=(N-n.train))\n",
    "  n.test <- nrow(test.all)/ K  # size of each test set\n",
    "  test <- head(x=test.all,n=n.test)\n",
    "  y.test <- test$y\n",
    "  predictors.test <- test[, names(test)!= \"y\"]\n",
    "  \n",
    "  \n",
    "  # loop for 1) fitting each model using a train set, calculating respective test AUC \n",
    "  #          2) updating train and test data sets\n",
    "  #          3) calculating test AUC\n",
    "  \n",
    "  AUC.tree.appended <- c()\n",
    "  AUC.forest.appended <- c()\n",
    "  \n",
    "  for (i in 2:K){\n",
    "    \n",
    "    forest.fit <- randomForest(x=predictors.train, y=y.train)\n",
    "    y.matrix.forest <- predict(object=forest.fit, newdata=predictors.test, type=\"prob\")\n",
    "    y.data.forest <- cbind.data.frame(as.vector(y.matrix.forest[,2]), y.test)\n",
    "    AUC.forest.result <- auc(data=y.data.forest, response=y.data[,2], predictor=y.data[,1])\n",
    "    AUC.forest.appended <- append(AUC.forest.appended, AUC.forest.result)\n",
    "    \n",
    "    tree.fit <- rpart(y~., data=train, method=\"class\")\n",
    "    y.matrix.tree <- predict(tree.fit, newdata=test, type=\"prob\")\n",
    "    y.data.tree <- cbind.data.frame(as.vector(y.matrix.tree[,2]), y.test)\n",
    "    AUC.tree.result <- auc(data=y.data.tree, response=y.data[,2], predictor=y.data[,1])\n",
    "    AUC.tree.appended <- append(AUC.tree.appended, AUC.tree.result)\n",
    "    \n",
    "    if (nrow(test.all) >= n.test){\n",
    "      \n",
    "      train <- tail(train, -n.test)\n",
    "      train <- rbind(train, test)\n",
    "      test.all <- tail(test.all, -n.test)\n",
    "      test <- head(x=test.all,n=n.test)\n",
    "      y.train <- train$y\n",
    "      predictors.train <- train[, names(train)!= \"y\"]\n",
    "      y.test <- test$y\n",
    "      predictors.test <- test[, names(test)!= \"y\"]\n",
    "      \n",
    "    }\n",
    "    \n",
    "  }\n",
    "  \n",
    "  ave.AUC.forest <- mean(AUC.forest.appended)\n",
    "  ave.AUC.tree <- mean(AUC.tree.appended)\n",
    "  \n",
    "  df.AUC <- data.frame(matrix(ncol=2, nrow=2))\n",
    "  colnames(df.AUC) <- c(\"Method\",\"ave.AUC\")\n",
    "  df.AUC$Method[1] <- \"Decision Tree\"\n",
    "  df.AUC$Method[2] <- \"Random Forest\"\n",
    "  df.AUC$ave.AUC[1] <- ave.AUC.tree\n",
    "  df.AUC$ave.AUC[2] <- ave.AUC.forest\n",
    "  \n",
    "  return(df.AUC)\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rolling window evaluation results\n",
    "emp.results <- rolling.window.evaluation(dataset=df,K=10, test.size = 0.10)\n",
    "print(emp.results)\n",
    "\n",
    "# fit Classification tree and get summary\n",
    "emp.tree.fit <- rpart(y~., data=df, method=\"class\")\n",
    "summary(emp.tree.fit)\n",
    "plot(emp.fit.tree)\n",
    "text(emp.fit.tree)\n",
    "\n",
    "# fit Random Forest\n",
    "df.response <- df$y\n",
    "df$y <- NULL\n",
    "forest.fit <- randomForest(x=df, y=df.response)\n",
    "importance(forest.fit)\n",
    "varImpPlot(forest.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected after simulation analysis Random Forest and Decision Tree show the same result. For comparison reasons the summary of fitted Decision Tree is also included. Given the current data set it makes more sense to use Decision Tree as it offers both high interpreatability and prediction accuracy. The aim of this project was to show that for some data sets Random Forest could be a good balance of prediction accuracy and interpretability, and even though the first part with regards to prediction accuracy did not work out well,I still decided to demonstrate relative interpretability of Random Forest. I fitted the model for the whole set and printed the model information that includes the most important features and the graph for better visualisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. References <a class=\"anchor\" id=\"refernces\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Arellano-Valle R. B. and Azzalini A. The centred parameterization and related quantities of the skew-t distribution. Journal of Multivariate Analysis, Elsevier, 113: 73-90 January 2013\n",
    "* S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014\n",
    "* [Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
